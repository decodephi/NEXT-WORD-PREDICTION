{"cells":[{"cell_type":"markdown","metadata":{"id":"sBJZBC0I3_uk"},"source":["##  Next Word Prediction Using LSTM\n","#### Project Overview:\n","\n","This project aims to develop a deep learning model for predicting the next word in a given sequence of words. The model is built using Long Short-Term Memory (LSTM) networks, which are well-suited for sequence prediction tasks. The project includes the following steps:\n","\n","1- Data Collection: We use the text of Shakespeare's \"Hamlet\" as our dataset. This rich, complex text provides a good challenge for our model.\n","\n","2- Data Preprocessing: The text data is tokenized, converted into sequences, and padded to ensure uniform input lengths. The sequences are then split into training and testing sets.\n","\n","3- Model Building: An LSTM model is constructed with an embedding layer, two LSTM layers, and a dense output layer with a softmax activation function to predict the probability of the next word.\n","\n","4- Model Training: The model is trained using the prepared sequences, with early stopping implemented to prevent overfitting. Early stopping monitors the validation loss and stops training when the loss stops improving.\n","\n","5- Model Evaluation: The model is evaluated using a set of example sentences to test its ability to predict the next word accurately.\n","\n","6- Deployment: A Streamlit web application is developed to allow users to input a sequence of words and get the predicted next word in real-time."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1758820998792,"user":{"displayName":"Avanti Sharma","userId":"17226617004774171125"},"user_tz":-330},"id":"o0ejJ6QR1q6o","outputId":"b7ea1fc0-0309-4181-8c83-76f718bc01b7"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n"]}],"source":["import nltk\n","nltk.download('gutenberg')\n","from nltk.corpus import gutenberg\n","import  pandas as pd\n","\n","## load the dataset\n","data=gutenberg.raw('shakespeare-hamlet.txt')\n","## save to a file\n","with open('/content/next_predicter/hamlet.txt','w') as file:\n","    file.write(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3128,"status":"ok","timestamp":1758821391128,"user":{"displayName":"Avanti Sharma","userId":"17226617004774171125"},"user_tz":-330},"id":"3HfiqXQi4yh-","outputId":"f61a1a90-d7e1-497f-8928-b3c308619778"},"outputs":[{"data":{"text/plain":["4818"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Data Preprocessing\n","\n","import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","#load the dataset\n","with open ('/content/next_predicter/hamlet.txt') as file:\n","    text = file.read().lower()\n","\n","#Tokenizer the txt\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","\n","#totalwords\n","total_words = len(tokenizer.word_index)+1\n","total_words"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":101,"status":"ok","timestamp":1758821431807,"user":{"displayName":"Avanti Sharma","userId":"17226617004774171125"},"user_tz":-330},"id":"TBRUk-QR5ad5","outputId":"43f6b76f-5079-408c-f339-09189839f055"},"outputs":[{"data":{"text/plain":["{'the': 1,\n"," 'and': 2,\n"," 'to': 3,\n"," 'of': 4,\n"," 'i': 5,\n"," 'you': 6,\n"," 'a': 7,\n"," 'my': 8,\n"," 'it': 9,\n"," 'in': 10,\n"," 'that': 11,\n"," 'ham': 12,\n"," 'is': 13,\n"," 'not': 14,\n"," 'his': 15,\n"," 'this': 16,\n"," 'with': 17,\n"," 'your': 18,\n"," 'but': 19,\n"," 'for': 20,\n"," 'me': 21,\n"," 'lord': 22,\n"," 'as': 23,\n"," 'what': 24,\n"," 'he': 25,\n"," 'be': 26,\n"," 'so': 27,\n"," 'him': 28,\n"," 'haue': 29,\n"," 'king': 30,\n"," 'will': 31,\n"," 'no': 32,\n"," 'our': 33,\n"," 'we': 34,\n"," 'on': 35,\n"," 'are': 36,\n"," 'if': 37,\n"," 'all': 38,\n"," 'then': 39,\n"," 'shall': 40,\n"," 'by': 41,\n"," 'thou': 42,\n"," 'come': 43,\n"," 'or': 44,\n"," 'hamlet': 45,\n"," 'good': 46,\n"," 'do': 47,\n"," 'hor': 48,\n"," 'her': 49,\n"," 'let': 50,\n"," 'now': 51,\n"," 'thy': 52,\n"," 'how': 53,\n"," 'more': 54,\n"," 'they': 55,\n"," 'from': 56,\n"," 'enter': 57,\n"," 'at': 58,\n"," 'was': 59,\n"," 'oh': 60,\n"," 'like': 61,\n"," 'most': 62,\n"," 'there': 63,\n"," 'well': 64,\n"," 'know': 65,\n"," 'selfe': 66,\n"," 'would': 67,\n"," 'them': 68,\n"," 'loue': 69,\n"," 'may': 70,\n"," \"'tis\": 71,\n"," 'vs': 72,\n"," 'sir': 73,\n"," 'qu': 74,\n"," 'which': 75,\n"," 'did': 76,\n"," 'why': 77,\n"," 'laer': 78,\n"," 'giue': 79,\n"," 'thee': 80,\n"," 'ile': 81,\n"," 'must': 82,\n"," 'hath': 83,\n"," 'ophe': 84,\n"," 'speake': 85,\n"," 'out': 86,\n"," 'make': 87,\n"," 'should': 88,\n"," 'where': 89,\n"," 'too': 90,\n"," 'an': 91,\n"," 'am': 92,\n"," 'such': 93,\n"," 'say': 94,\n"," 'when': 95,\n"," 'vpon': 96,\n"," 'father': 97,\n"," 'doe': 98,\n"," 'very': 99,\n"," 'pol': 100,\n"," 'go': 101,\n"," 'their': 102,\n"," 'one': 103,\n"," 'man': 104,\n"," 'see': 105,\n"," 'some': 106,\n"," 'heere': 107,\n"," 'had': 108,\n"," 'heauen': 109,\n"," 'time': 110,\n"," 'mine': 111,\n"," 'these': 112,\n"," 'she': 113,\n"," 'much': 114,\n"," 'tell': 115,\n"," 'rosin': 116,\n"," 'thinke': 117,\n"," 'play': 118,\n"," 'thus': 119,\n"," 'horatio': 120,\n"," 'who': 121,\n"," 'mother': 122,\n"," 'queene': 123,\n"," 'night': 124,\n"," 'o': 125,\n"," 'polon': 126,\n"," 'yet': 127,\n"," 'vp': 128,\n"," 'death': 129,\n"," 'laertes': 130,\n"," 'againe': 131,\n"," 'can': 132,\n"," 'both': 133,\n"," \"th'\": 134,\n"," 'soule': 135,\n"," 'take': 136,\n"," 'life': 137,\n"," 'nor': 138,\n"," 'heare': 139,\n"," 'mar': 140,\n"," 'looke': 141,\n"," 'owne': 142,\n"," 'could': 143,\n"," 'heart': 144,\n"," 'dead': 145,\n"," 'might': 146,\n"," 'made': 147,\n"," 'clo': 148,\n"," 'hast': 149,\n"," 'downe': 150,\n"," 'pray': 151,\n"," 'ophelia': 152,\n"," 'nothing': 153,\n"," 'away': 154,\n"," 'whose': 155,\n"," 'doth': 156,\n"," 'other': 157,\n"," 'cannot': 158,\n"," 'leaue': 159,\n"," 'indeed': 160,\n"," 'into': 161,\n"," 'nay': 162,\n"," 'god': 163,\n"," 'head': 164,\n"," 'were': 165,\n"," 'matter': 166,\n"," 'thing': 167,\n"," 'hold': 168,\n"," 'day': 169,\n"," 'world': 170,\n"," 'nature': 171,\n"," 'neuer': 172,\n"," 'comes': 173,\n"," 'done': 174,\n"," 'exeunt': 175,\n"," 'call': 176,\n"," 'two': 177,\n"," 'true': 178,\n"," 'though': 179,\n"," 'sweet': 180,\n"," 'put': 181,\n"," 'set': 182,\n"," 'ghost': 183,\n"," 'euen': 184,\n"," 'earth': 185,\n"," 'feare': 186,\n"," 'madnesse': 187,\n"," 'mad': 188,\n"," 'seene': 189,\n"," 'eyes': 190,\n"," 'against': 191,\n"," 'faire': 192,\n"," 'denmarke': 193,\n"," 'those': 194,\n"," \"o're\": 195,\n"," 'polonius': 196,\n"," 'deere': 197,\n"," 'fathers': 198,\n"," 'sonne': 199,\n"," 'poore': 200,\n"," 'himselfe': 201,\n"," 'follow': 202,\n"," 'guild': 203,\n"," 'england': 204,\n"," 'friends': 205,\n"," 'once': 206,\n"," 'hand': 207,\n"," 'shew': 208,\n"," 'about': 209,\n"," \"i'th'\": 210,\n"," 'off': 211,\n"," 'within': 212,\n"," 'till': 213,\n"," 'great': 214,\n"," 'meanes': 215,\n"," 'words': 216,\n"," 'players': 217,\n"," 'exit': 218,\n"," 'part': 219,\n"," 'still': 220,\n"," 'does': 221,\n"," 'hee': 222,\n"," 'osr': 223,\n"," 'long': 224,\n"," 'before': 225,\n"," 'beleeue': 226,\n"," 'any': 227,\n"," 'old': 228,\n"," 'thoughts': 229,\n"," 'first': 230,\n"," 'eare': 231,\n"," 'keepe': 232,\n"," 'goe': 233,\n"," 'end': 234,\n"," 'guildensterne': 235,\n"," 'welcome': 236,\n"," 'while': 237,\n"," 'art': 238,\n"," 'noble': 239,\n"," 'body': 240,\n"," 'bee': 241,\n"," 'daughter': 242,\n"," 'speech': 243,\n"," 'makes': 244,\n"," \"there's\": 245,\n"," 'sword': 246,\n"," 'stand': 247,\n"," 'liue': 248,\n"," \"that's\": 249,\n"," 'farewell': 250,\n"," 'kin': 251,\n"," 'ere': 252,\n"," 'marry': 253,\n"," 'betweene': 254,\n"," 'many': 255,\n"," 'since': 256,\n"," 'watch': 257,\n"," \"ha's\": 258,\n"," 'therefore': 259,\n"," 'question': 260,\n"," 'thought': 261,\n"," 'heard': 262,\n"," 'spirit': 263,\n"," 'eye': 264,\n"," 'better': 265,\n"," 'thine': 266,\n"," 'tongue': 267,\n"," 'drinke': 268,\n"," 'youth': 269,\n"," 'sent': 270,\n"," 'graue': 271,\n"," 'rest': 272,\n"," 'bed': 273,\n"," 'last': 274,\n"," 'same': 275,\n"," 'marke': 276,\n"," 'gone': 277,\n"," 'without': 278,\n"," 'state': 279,\n"," \"is't\": 280,\n"," 'goes': 281,\n"," 'fortinbras': 282,\n"," 'vse': 283,\n"," 'grace': 284,\n"," 'euer': 285,\n"," 'finde': 286,\n"," 'gertrude': 287,\n"," 'beare': 288,\n"," 'little': 289,\n"," 'breath': 290,\n"," \"wee'l\": 291,\n"," 'saw': 292,\n"," 'beene': 293,\n"," 'none': 294,\n"," 'vertue': 295,\n"," 'else': 296,\n"," 'said': 297,\n"," 'after': 298,\n"," 'reynol': 299,\n"," 'cause': 300,\n"," 'forme': 301,\n"," 'something': 302,\n"," 'ayre': 303,\n"," 'farre': 304,\n"," 'selues': 305,\n"," 'purpose': 306,\n"," 'further': 307,\n"," 'reason': 308,\n"," 'friend': 309,\n"," 'madam': 310,\n"," 'remember': 311,\n"," 'faith': 312,\n"," 'gentlemen': 313,\n"," 'word': 314,\n"," 'foule': 315,\n"," 'winde': 316,\n"," 'meane': 317,\n"," 'bring': 318,\n"," 'fit': 319,\n"," 'blood': 320,\n"," 'helpe': 321,\n"," 'honest': 322,\n"," 'stay': 323,\n"," \"in't\": 324,\n"," 'being': 325,\n"," 'fire': 326,\n"," 'things': 327,\n"," \"what's\": 328,\n"," 'newes': 329,\n"," 'best': 330,\n"," 'kinde': 331,\n"," 'excellent': 332,\n"," 'each': 333,\n"," 'sleepe': 334,\n"," 'way': 335,\n"," 'please': 336,\n"," 'free': 337,\n"," 'reuenge': 338,\n"," 'villaine': 339,\n"," 'right': 340,\n"," 'ha': 341,\n"," 'passion': 342,\n"," 'rosincrance': 343,\n"," 'dost': 344,\n"," 'verie': 345,\n"," 'barn': 346,\n"," 'marcellus': 347,\n"," 'men': 348,\n"," 'peace': 349,\n"," 'together': 350,\n"," 'full': 351,\n"," 'voyce': 352,\n"," 'oft': 353,\n"," 'greefe': 354,\n"," \"'twere\": 355,\n"," 'late': 356,\n"," 'businesse': 357,\n"," 'doubt': 358,\n"," 'alone': 359,\n"," 'minde': 360,\n"," 'heauens': 361,\n"," 'face': 362,\n"," 'hell': 363,\n"," 'ye': 364,\n"," 'second': 365,\n"," 'iudgement': 366,\n"," 'giuen': 367,\n"," 'command': 368,\n"," 'action': 369,\n"," \"let's\": 370,\n"," 'murther': 371,\n"," 'guil': 372,\n"," 'lady': 373,\n"," 'fortune': 374,\n"," 'mee': 375,\n"," 'pyrrhus': 376,\n"," 'answer': 377,\n"," 'get': 378,\n"," 'thankes': 379,\n"," 'goodnight': 380,\n"," 'eares': 381,\n"," 'breake': 382,\n"," 'hora': 383,\n"," 'strange': 384,\n"," 'young': 385,\n"," 'walke': 386,\n"," 'brothers': 387,\n"," 'seeme': 388,\n"," 'name': 389,\n"," 'fellow': 390,\n"," 'act': 391,\n"," 'hands': 392,\n"," 'armes': 393,\n"," 'deare': 394,\n"," 'neere': 395,\n"," 'phrase': 396,\n"," 'draw': 397,\n"," 'gho': 398,\n"," 'alas': 399,\n"," 'ought': 400,\n"," 'offence': 401,\n"," 'sweare': 402,\n"," 'worke': 403,\n"," 'gentleman': 404,\n"," 'fine': 405,\n"," 'three': 406,\n"," 'barnardo': 407,\n"," 'fran': 408,\n"," 'ground': 409,\n"," 'sight': 410,\n"," 'sit': 411,\n"," 'maiesty': 412,\n"," 'pale': 413,\n"," \"on't\": 414,\n"," 'fell': 415,\n"," 'lost': 416,\n"," 'soft': 417,\n"," 'power': 418,\n"," 'yong': 419,\n"," 'duty': 420,\n"," 'whole': 421,\n"," 'woe': 422,\n"," 'ioy': 423,\n"," 'wife': 424,\n"," 'came': 425,\n"," 'queen': 426,\n"," 'seeke': 427,\n"," 'common': 428,\n"," 'seemes': 429,\n"," 'blacke': 430,\n"," 'kings': 431,\n"," 'teares': 432,\n"," 'top': 433,\n"," 'fashion': 434,\n"," 'deed': 435,\n"," 'euery': 436,\n"," 'light': 437,\n"," 'custome': 438,\n"," 'borne': 439,\n"," 'wilt': 440,\n"," 'hither': 441,\n"," 'lay': 442,\n"," 'another': 443,\n"," 'ouer': 444,\n"," 'age': 445,\n"," 'thousand': 446,\n"," 'fall': 447,\n"," 'lye': 448,\n"," 'conscience': 449,\n"," 'husband': 450,\n"," 'bar': 451,\n"," 'lookes': 452,\n"," 'charge': 453,\n"," 'knowne': 454,\n"," 'law': 455,\n"," 'bin': 456,\n"," 'sound': 457,\n"," 'sister': 458,\n"," 'memory': 459,\n"," 'brother': 460,\n"," 'beseech': 461,\n"," 'lesse': 462,\n"," 'dust': 463,\n"," 'through': 464,\n"," 'shewes': 465,\n"," 'desire': 466,\n"," 'obey': 467,\n"," 'woman': 468,\n"," 'almost': 469,\n"," 'grow': 470,\n"," 'here': 471,\n"," 'shame': 472,\n"," 'giues': 473,\n"," \"too't\": 474,\n"," 'takes': 475,\n"," 'table': 476,\n"," 'sure': 477,\n"," 'musicke': 478,\n"," 'letters': 479,\n"," 'hamlets': 480,\n"," 'hope': 481,\n"," 'receiue': 482,\n"," 'maiestie': 483,\n"," 'thanke': 484,\n"," 'gaue': 485,\n"," 'bad': 486,\n"," 'wee': 487,\n"," 'ore': 488,\n"," 'noise': 489,\n"," 'times': 490,\n"," 'cold': 491,\n"," 'bid': 492,\n"," 'dane': 493,\n"," 'place': 494,\n"," 'peece': 495,\n"," 'buried': 496,\n"," 'cast': 497,\n"," 'hot': 498,\n"," 'list': 499,\n"," 'wrong': 500,\n"," 'sea': 501,\n"," 'truth': 502,\n"," 'sayes': 503,\n"," 'season': 504,\n"," 'gracious': 505,\n"," 'dumbe': 506,\n"," 'loues': 507,\n"," 'sorrow': 508,\n"," 'marriage': 509,\n"," 'writ': 510,\n"," 'mouth': 511,\n"," 'pardon': 512,\n"," 'note': 513,\n"," 'backe': 514,\n"," 'lordship': 515,\n"," 'mothers': 516,\n"," 'beard': 517,\n"," 'fare': 518,\n"," 'seruice': 519,\n"," 'withall': 520,\n"," 'maid': 521,\n"," 'enough': 522,\n"," 'effect': 523,\n"," 'double': 524,\n"," 'neither': 525,\n"," 'false': 526,\n"," 'vnderstand': 527,\n"," 'circumstance': 528,\n"," 'foole': 529,\n"," 'vowes': 530,\n"," 'keepes': 531,\n"," 'shape': 532,\n"," 'dayes': 533,\n"," 'fat': 534,\n"," 'crowne': 535,\n"," 'wits': 536,\n"," 'damned': 537,\n"," 'ho': 538,\n"," 'needs': 539,\n"," 'touch': 540,\n"," 'ranke': 541,\n"," 'generall': 542,\n"," 'moue': 543,\n"," 'home': 544,\n"," 'ill': 545,\n"," 'round': 546,\n"," 'fortunes': 547,\n"," 'laugh': 548,\n"," 'yours': 549,\n"," \"he's\": 550,\n"," 'honor': 551,\n"," 'begin': 552,\n"," 'anon': 553,\n"," 'proofe': 554,\n"," 'gods': 555,\n"," 'quicke': 556,\n"," 'dangerous': 557,\n"," 'christian': 558,\n"," 'danish': 559,\n"," 'poyson': 560,\n"," 'begge': 561,\n"," 'wager': 562,\n"," \"drown'd\": 563,\n"," 'water': 564,\n"," 'scull': 565,\n"," 'houre': 566,\n"," 'twelue': 567,\n"," 'quiet': 568,\n"," 'course': 569,\n"," 'sometimes': 570,\n"," 'march': 571,\n"," 'look': 572,\n"," 'norwey': 573,\n"," 'particular': 574,\n"," 'land': 575,\n"," \"do's\": 576,\n"," 'vnto': 577,\n"," 'speak': 578,\n"," 'spirits': 579,\n"," 'cocke': 580,\n"," 'guilty': 581,\n"," \"'gainst\": 582,\n"," 'wholsome': 583,\n"," 'lords': 584,\n"," 'kingdome': 585,\n"," 'freely': 586,\n"," 'dreame': 587,\n"," 'told': 588,\n"," 'loose': 589,\n"," 'dread': 590,\n"," 'returne': 591,\n"," 'france': 592,\n"," 'confesse': 593,\n"," 'dye': 594,\n"," 'visage': 595,\n"," 'truly': 596,\n"," 'bound': 597,\n"," 'prythee': 598,\n"," 'health': 599,\n"," 'flesh': 600,\n"," 'fie': 601,\n"," 'beast': 602,\n"," 'discourse': 603,\n"," 'longer': 604,\n"," 'wicked': 605,\n"," 'disposition': 606,\n"," 'report': 607,\n"," 'teach': 608,\n"," 'forth': 609,\n"," 'thinkes': 610,\n"," 'tis': 611,\n"," 'yes': 612,\n"," 'countenance': 613,\n"," 'perchance': 614,\n"," 'warrant': 615,\n"," 'silence': 616,\n"," 'perhaps': 617,\n"," 'wisedome': 618,\n"," 'blessing': 619,\n"," 'dull': 620,\n"," 'mans': 621,\n"," 'audience': 622,\n"," \"you'l\": 623,\n"," 'making': 624,\n"," \"damn'd\": 625,\n"," 'soules': 626,\n"," 'cries': 627,\n"," 'desperate': 628,\n"," 'shalt': 629,\n"," 'prison': 630,\n"," 'went': 631,\n"," 'naturall': 632,\n"," 'holds': 633,\n"," 'sodaine': 634,\n"," 'adue': 635,\n"," 'braine': 636,\n"," 'knaue': 637,\n"," 'point': 638,\n"," 'vnder': 639,\n"," 'mercy': 640,\n"," 'lacke': 641,\n"," \"heere's\": 642,\n"," \"in's\": 643,\n"," 'tooke': 644,\n"," 'arme': 645,\n"," 'brought': 646,\n"," 'dutie': 647,\n"," 'found': 648,\n"," 'whereon': 649,\n"," 'commission': 650,\n"," 'passe': 651,\n"," 'vilde': 652,\n"," 'short': 653,\n"," 'try': 654,\n"," 'behinde': 655,\n"," 'presently': 656,\n"," 'slaue': 657,\n"," 'saue': 658,\n"," 'ambition': 659,\n"," 'sing': 660,\n"," 'themselues': 661,\n"," 'braines': 662,\n"," \"'twas\": 663,\n"," 'onely': 664,\n"," 'french': 665,\n"," 'treason': 666,\n"," 'morrow': 667,\n"," 'conceit': 668,\n"," 'drowne': 669,\n"," 'fellowes': 670,\n"," 'hoa': 671,\n"," 'patience': 672,\n"," 'halfe': 673,\n"," 'yeare': 674,\n"," 'forgot': 675,\n"," 'hence': 676,\n"," \"doo't\": 677,\n"," \"e'ene\": 678,\n"," 'slaine': 679,\n"," 'sense': 680,\n"," 'buriall': 681,\n"," 'alexander': 682,\n"," 'osricke': 683,\n"," 'carriages': 684,\n"," 'foyles': 685,\n"," 'hit': 686,\n"," 'tragedie': 687,\n"," 'sicke': 688,\n"," 'meet': 689,\n"," 'twice': 690,\n"," \"'twill\": 691,\n"," 'nights': 692,\n"," 'starre': 693,\n"," \"t'\": 694,\n"," 'figure': 695,\n"," 'wonder': 696,\n"," 'warlike': 697,\n"," 'cannon': 698,\n"," 'toward': 699,\n"," 'image': 700,\n"," 'norway': 701,\n"," \"seal'd\": 702,\n"," 'lands': 703,\n"," 'stood': 704,\n"," \"return'd\": 705,\n"," 'strong': 706,\n"," 'motiue': 707,\n"," 'ease': 708,\n"," 'treasure': 709,\n"," 'stop': 710,\n"," 'violence': 711,\n"," 'trumpet': 712,\n"," 'whether': 713,\n"," 'heerein': 714,\n"," 'wherein': 715,\n"," 'impart': 716,\n"," 'morning': 717,\n"," 'attendant': 718,\n"," 'brow': 719,\n"," 'discretion': 720,\n"," 'followes': 721,\n"," 'frame': 722,\n"," 'cornelius': 723,\n"," 'giuing': 724,\n"," 'commend': 725,\n"," \"would'st\": 726,\n"," 'natiue': 727,\n"," 'fauour': 728,\n"," 'hang': 729,\n"," 'colour': 730,\n"," 'show': 731,\n"," 'fault': 732,\n"," 'throw': 733,\n"," 'lose': 734,\n"," 'gentle': 735,\n"," 'sits': 736,\n"," 'growes': 737,\n"," 'visit': 738,\n"," 'growne': 739,\n"," 'vnkle': 740,\n"," 'left': 741,\n"," 'speed': 742,\n"," 'incestuous': 743,\n"," 'glad': 744,\n"," 'forget': 745,\n"," 'thrift': 746,\n"," \"arm'd\": 747,\n"," 'thrice': 748,\n"," 'length': 749,\n"," 'kept': 750,\n"," 'knew': 751,\n"," 'answere': 752,\n"," 'honour': 753,\n"," 'wide': 754,\n"," 'force': 755,\n"," 'affection': 756,\n"," 'shot': 757,\n"," 'danger': 758,\n"," 'moone': 759,\n"," 'safety': 760,\n"," 'lies': 761,\n"," 'buy': 762,\n"," 'aboue': 763,\n"," 'humbly': 764,\n"," 'tend': 765,\n"," 'ist': 766,\n"," 'bloud': 767,\n"," 'heate': 768,\n"," 'bones': 769,\n"," 'base': 770,\n"," 'horrible': 771,\n"," 'imagination': 772,\n"," 'direct': 773,\n"," 'lend': 774,\n"," 'hearing': 775,\n"," 'certaine': 776,\n"," 'fast': 777,\n"," 'house': 778,\n"," 'tale': 779,\n"," 'start': 780,\n"," 'vnnaturall': 781,\n"," \"it's\": 782,\n"," 'sleeping': 783,\n"," 'wit': 784,\n"," 'gifts': 785,\n"," 'seeming': 786,\n"," 'wil': 787,\n"," 'court': 788,\n"," 'cursed': 789,\n"," 'instant': 790,\n"," 'bosome': 791,\n"," 'distracted': 792,\n"," 'yea': 793,\n"," 'past': 794,\n"," 'booke': 795,\n"," 'think': 796,\n"," 'secret': 797,\n"," 'already': 798,\n"," 'stage': 799,\n"," 'seeing': 800,\n"," 'fingers': 801,\n"," 'reynoldo': 802,\n"," 'drift': 803,\n"," 'liberty': 804,\n"," 'closes': 805,\n"," 'consequence': 806,\n"," 'chamber': 807,\n"," \"turn'd\": 808,\n"," 'extasie': 809,\n"," 'violent': 810,\n"," 'meant': 811,\n"," 'hide': 812,\n"," 'whom': 813,\n"," 'rather': 814,\n"," 'white': 815,\n"," 'awhile': 816,\n"," 'idle': 817,\n"," 'thence': 818,\n"," 'bene': 819,\n"," 'foure': 820,\n"," 'honestie': 821,\n"," 'either': 822,\n"," 'count': 823,\n"," 'dreames': 824,\n"," 'bodies': 825,\n"," 'comming': 826,\n"," 'shal': 827,\n"," 'player': 828,\n"," 'flourish': 829,\n"," 'asse': 830,\n"," 'heauy': 831,\n"," 'masters': 832,\n"," '1': 833,\n"," 'others': 834,\n"," 'modestie': 835,\n"," 'cunning': 836,\n"," \"lou'd\": 837,\n"," 'horse': 838,\n"," 'priam': 839,\n"," 'new': 840,\n"," 'sleepes': 841,\n"," 'hecuba': 842,\n"," 'mortall': 843,\n"," 'weepe': 844,\n"," 'pate': 845,\n"," 'diuell': 846,\n"," 'blame': 847,\n"," 'flye': 848,\n"," 'turne': 849,\n"," 'beautie': 850,\n"," 'acte': 851,\n"," 'nunnery': 852,\n"," 'snow': 853,\n"," 'rose': 854,\n"," 'quite': 855,\n"," 'ladies': 856,\n"," 'wretched': 857,\n"," 'send': 858,\n"," 'weare': 859,\n"," \"kill'd\": 860,\n"," 'kill': 861,\n"," 'maker': 862,\n"," 'prologue': 863,\n"," 'shortly': 864,\n"," 'begun': 865,\n"," 'lights': 866,\n"," 'heeles': 867,\n"," 'doore': 868,\n"," 'meete': 869,\n"," 'stronger': 870,\n"," 'whereto': 871,\n"," 'next': 872,\n"," 'messenger': 873,\n"," 'choose': 874,\n"," 'bore': 875,\n"," 'spade': 876,\n"," 'gallowes': 877,\n"," 'sings': 878,\n"," 'rites': 879,\n"," \"woo't\": 880,\n"," 'cup': 881,\n"," 'vnfold': 882,\n"," 'bitter': 883,\n"," 'guard': 884,\n"," 'mouse': 885,\n"," \"appear'd\": 886,\n"," 'saies': 887,\n"," 'touching': 888,\n"," 'along': 889,\n"," 'appeare': 890,\n"," 'beating': 891,\n"," 'offended': 892,\n"," 'ambitious': 893,\n"," 'iust': 894,\n"," 'knowes': 895,\n"," 'subiect': 896,\n"," 'sore': 897,\n"," 'least': 898,\n"," 'valiant': 899,\n"," 'recouer': 900,\n"," 'termes': 901,\n"," 'maine': 902,\n"," 'loe': 903,\n"," 'offer': 904,\n"," 'present': 905,\n"," 'dew': 906,\n"," 'high': 907,\n"," 'hill': 908,\n"," 'aduice': 909,\n"," 'consent': 910,\n"," 'scena': 911,\n"," 'greene': 912,\n"," 'hearts': 913,\n"," 'wisest': 914,\n"," 'remembrance': 915,\n"," 'funerall': 916,\n"," 'delight': 917,\n"," 'affaire': 918,\n"," 'thinking': 919,\n"," 'voltemand': 920,\n"," 'vncle': 921,\n"," 'heartily': 922,\n"," 'bend': 923,\n"," 'bow': 924,\n"," 'cosin': 925,\n"," 'clouds': 926,\n"," 'sun': 927,\n"," 'liues': 928,\n"," 'passing': 929,\n"," 'formes': 930,\n"," 'vnderstanding': 931,\n"," 'sence': 932,\n"," 'coarse': 933,\n"," 'dyed': 934,\n"," 'beares': 935,\n"," 'wittenberg': 936,\n"," 'courtier': 937,\n"," 'louing': 938,\n"," 'smiling': 939,\n"," 'manet': 940,\n"," 'fixt': 941,\n"," 'flat': 942,\n"," 'vses': 943,\n"," 'married': 944,\n"," 'hercules': 945,\n"," 'salt': 946,\n"," 'change': 947,\n"," 'deepe': 948,\n"," 'mock': 949,\n"," 'hard': 950,\n"," 'tables': 951,\n"," 'goodly': 952,\n"," 'admiration': 953,\n"," 'deliuer': 954,\n"," 'cap': 955,\n"," 'appeares': 956,\n"," 'whilst': 957,\n"," 'dreadfull': 958,\n"," 'third': 959,\n"," 'sirs': 960,\n"," 'foote': 961,\n"," 'staid': 962,\n"," 'assume': 963,\n"," 'person': 964,\n"," 'mens': 965,\n"," 'choyce': 966,\n"," 'beauty': 967,\n"," 'spring': 968,\n"," 'aboord': 969,\n"," 'saile': 970,\n"," 'few': 971,\n"," 'steele': 972,\n"," 'entertainment': 973,\n"," 'censure': 974,\n"," 'rich': 975,\n"," 'generous': 976,\n"," 'edge': 977,\n"," 'tenders': 978,\n"," 'pay': 979,\n"," 'tender': 980,\n"," 'honourable': 981,\n"," 'presence': 982,\n"," 'meere': 983,\n"," 'wayes': 984,\n"," 'hower': 985,\n"," 'strooke': 986,\n"," 'wont': 987,\n"," \"honour'd\": 988,\n"," 'angels': 989,\n"," 'royall': 990,\n"," 'burst': 991,\n"," 'ignorance': 992,\n"," 'shake': 993,\n"," 'wherefore': 994,\n"," 'tempt': 995,\n"," 'lets': 996,\n"," 'lead': 997,\n"," 'crimes': 998,\n"," 'starres': 999,\n"," 'stirre': 1000,\n"," ...}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.word_index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ihG7AeXT5aai"},"outputs":[],"source":["#create input sequence\n","input_sequences = []\n","for line in text.split('\\n'):\n","  token_list = tokenizer.texts_to_sequences([line])[0]\n","  for i in range(1, len(token_list)):\n","    n_gram_sque = token_list[: i+1]\n","    input_sequences.append(n_gram_sque)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1758821676251,"user":{"displayName":"Avanti Sharma","userId":"17226617004774171125"},"user_tz":-330},"id":"C_7A2-ze5aXX","outputId":"934e51d2-f1f8-4565-ee3d-a75adf5b2748"},"outputs":[{"data":{"text/plain":["[[1, 687],\n"," [1, 687, 4],\n"," [1, 687, 4, 45],\n"," [1, 687, 4, 45, 41],\n"," [1, 687, 4, 45, 41, 1886],\n"," [1, 687, 4, 45, 41, 1886, 1887],\n"," [1, 687, 4, 45, 41, 1886, 1887, 1888],\n"," [1180, 1889],\n"," [1180, 1889, 1890],\n"," [1180, 1889, 1890, 1891]]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["input_sequences[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1758821902978,"user":{"displayName":"Avanti Sharma","userId":"17226617004774171125"},"user_tz":-330},"id":"W-8A5qDv5aUv","outputId":"5e89b637-a94e-429b-da76-6e3d9113985e"},"outputs":[{"data":{"text/plain":["14"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# pad sequence\n","max_sequence_len = max([len(x) for x in input_sequences])\n","max_sequence_len"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45,"status":"ok","timestamp":1758822038707,"user":{"displayName":"Avanti Sharma","userId":"17226617004774171125"},"user_tz":-330},"id":"LNOC_6fv9Gje","outputId":"353e7500-9ef8-44c3-8700-d4e123d87891"},"outputs":[{"data":{"text/plain":["array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1, 687,\n","         4], dtype=int32)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding = 'pre'))\n","input_sequences[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjclWwC19gBA"},"outputs":[],"source":["#create predictor and label\n","import tensorflow as tf\n","x,y = input_sequences[:, :-1], input_sequences[:, -1]\n","y = tf.keras.utils.to_categorical(y, num_classes = total_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1758822255149,"user":{"displayName":"Avanti Sharma","userId":"17226617004774171125"},"user_tz":-330},"id":"h2w-3ArB-abn","outputId":"85ebeb90-ecde-4fff-b0b1-80ee44948d80"},"outputs":[{"data":{"text/plain":["array([[   0,    0,    0, ...,    0,    0,    1],\n","       [   0,    0,    0, ...,    0,    1,  687],\n","       [   0,    0,    0, ...,    1,  687,    4],\n","       ...,\n","       [   0,    0,    0, ...,  687,    4,   45],\n","       [   0,    0,    0, ...,    4,   45, 1047],\n","       [   0,    0,    0, ...,   45, 1047,    4]], dtype=int32)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1758822295914,"user":{"displayName":"Avanti Sharma","userId":"17226617004774171125"},"user_tz":-330},"id":"9EiH7vuh9f9o","outputId":"93281d30-4f2e-4941-f09e-1638f0ccbe87"},"outputs":[{"data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uqgvm8hy-mep"},"outputs":[],"source":["x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"elapsed":61,"status":"ok","timestamp":1758823094426,"user":{"displayName":"Avanti Sharma","userId":"17226617004774171125"},"user_tz":-330},"id":"O3aPLIeE-9t9","outputId":"118a7f41-3cb2-4c4a-b416-2a90eaab1c5e"},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"sequential_3\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["#Train and test the model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n","\n","#Define the model\n","model = Sequential()\n","model.add(Embedding(total_words, 100, input_length = max_sequence_len-1))\n","model.add(LSTM(150, return_sequences = True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(100))\n","# model.add(Dense(total_words/2, activation = 'relu'))\n","model.add(Dense(total_words, activation = 'softmax'))\n","\n","\n","#compile the model\n","model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":101513,"status":"ok","timestamp":1758828799127,"user":{"displayName":"Avanti Sharma","userId":"17226617004774171125"},"user_tz":-330},"id":"hXheQHXcAkM1","outputId":"2549124c-7dc9-4b13-d427-5dff7f4810dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 83ms/step - accuracy: 0.0314 - loss: 7.1702 - val_accuracy: 0.0381 - val_loss: 6.6829\n","Epoch 2/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 75ms/step - accuracy: 0.0410 - loss: 6.4535 - val_accuracy: 0.0412 - val_loss: 6.7325\n","Epoch 3/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 76ms/step - accuracy: 0.0474 - loss: 6.2921 - val_accuracy: 0.0488 - val_loss: 6.7765\n","Epoch 4/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.0501 - loss: 6.1560 - val_accuracy: 0.0509 - val_loss: 6.7909\n","Epoch 5/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 78ms/step - accuracy: 0.0524 - loss: 6.0172 - val_accuracy: 0.0548 - val_loss: 6.8562\n","Epoch 6/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 77ms/step - accuracy: 0.0629 - loss: 5.8899 - val_accuracy: 0.0655 - val_loss: 6.8792\n","Epoch 7/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 81ms/step - accuracy: 0.0739 - loss: 5.7211 - val_accuracy: 0.0647 - val_loss: 6.9114\n","Epoch 8/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 78ms/step - accuracy: 0.0824 - loss: 5.5382 - val_accuracy: 0.0703 - val_loss: 6.9792\n","Epoch 9/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.0844 - loss: 5.4246 - val_accuracy: 0.0703 - val_loss: 7.0828\n","Epoch 10/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.0940 - loss: 5.3039 - val_accuracy: 0.0707 - val_loss: 7.1661\n","Epoch 11/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.0915 - loss: 5.1970 - val_accuracy: 0.0676 - val_loss: 7.2291\n","Epoch 12/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.1035 - loss: 5.0844 - val_accuracy: 0.0676 - val_loss: 7.3665\n","Epoch 13/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 70ms/step - accuracy: 0.1165 - loss: 4.9163 - val_accuracy: 0.0657 - val_loss: 7.4164\n","Epoch 14/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.1183 - loss: 4.8107 - val_accuracy: 0.0664 - val_loss: 7.5537\n","Epoch 15/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.1228 - loss: 4.6576 - val_accuracy: 0.0672 - val_loss: 7.6202\n","Epoch 16/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 69ms/step - accuracy: 0.1295 - loss: 4.5680 - val_accuracy: 0.0635 - val_loss: 7.7457\n","Epoch 17/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.1356 - loss: 4.4451 - val_accuracy: 0.0674 - val_loss: 7.9013\n","Epoch 18/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.1465 - loss: 4.3189 - val_accuracy: 0.0651 - val_loss: 8.0286\n","Epoch 19/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 71ms/step - accuracy: 0.1561 - loss: 4.2312 - val_accuracy: 0.0661 - val_loss: 8.1364\n","Epoch 20/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.1746 - loss: 4.1332 - val_accuracy: 0.0620 - val_loss: 8.3117\n","Epoch 21/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.1856 - loss: 4.0072 - val_accuracy: 0.0620 - val_loss: 8.3956\n","Epoch 22/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.2012 - loss: 3.9393 - val_accuracy: 0.0624 - val_loss: 8.5561\n","Epoch 23/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.2149 - loss: 3.8196 - val_accuracy: 0.0616 - val_loss: 8.6904\n","Epoch 24/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.2280 - loss: 3.7508 - val_accuracy: 0.0600 - val_loss: 8.7955\n","Epoch 25/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.2497 - loss: 3.6723 - val_accuracy: 0.0596 - val_loss: 8.9469\n","Epoch 26/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.2705 - loss: 3.5580 - val_accuracy: 0.0565 - val_loss: 9.0494\n","Epoch 27/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 75ms/step - accuracy: 0.2749 - loss: 3.5095 - val_accuracy: 0.0591 - val_loss: 9.1741\n","Epoch 28/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.2870 - loss: 3.4260 - val_accuracy: 0.0585 - val_loss: 9.2804\n","Epoch 29/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 73ms/step - accuracy: 0.3021 - loss: 3.3834 - val_accuracy: 0.0577 - val_loss: 9.4096\n","Epoch 30/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 71ms/step - accuracy: 0.3117 - loss: 3.3210 - val_accuracy: 0.0560 - val_loss: 9.5033\n","Epoch 31/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 70ms/step - accuracy: 0.3175 - loss: 3.2501 - val_accuracy: 0.0573 - val_loss: 9.6158\n","Epoch 32/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.3370 - loss: 3.1744 - val_accuracy: 0.0589 - val_loss: 9.7189\n","Epoch 33/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.3351 - loss: 3.1394 - val_accuracy: 0.0571 - val_loss: 9.8068\n","Epoch 34/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.3462 - loss: 3.0780 - val_accuracy: 0.0550 - val_loss: 9.8942\n","Epoch 35/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.3575 - loss: 3.0218 - val_accuracy: 0.0581 - val_loss: 9.9880\n","Epoch 36/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 70ms/step - accuracy: 0.3661 - loss: 2.9713 - val_accuracy: 0.0569 - val_loss: 10.0733\n","Epoch 37/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.3807 - loss: 2.9207 - val_accuracy: 0.0544 - val_loss: 10.1641\n","Epoch 38/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 71ms/step - accuracy: 0.3830 - loss: 2.8914 - val_accuracy: 0.0542 - val_loss: 10.2363\n","Epoch 39/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.4007 - loss: 2.8167 - val_accuracy: 0.0558 - val_loss: 10.3257\n","Epoch 40/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.4049 - loss: 2.7672 - val_accuracy: 0.0552 - val_loss: 10.3819\n","Epoch 41/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.4173 - loss: 2.7177 - val_accuracy: 0.0534 - val_loss: 10.4702\n","Epoch 42/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.4258 - loss: 2.6866 - val_accuracy: 0.0523 - val_loss: 10.5610\n","Epoch 43/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 72ms/step - accuracy: 0.4289 - loss: 2.6513 - val_accuracy: 0.0554 - val_loss: 10.6516\n","Epoch 44/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.4395 - loss: 2.6025 - val_accuracy: 0.0527 - val_loss: 10.7223\n","Epoch 45/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.4467 - loss: 2.5598 - val_accuracy: 0.0519 - val_loss: 10.7897\n","Epoch 46/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.4459 - loss: 2.5446 - val_accuracy: 0.0536 - val_loss: 10.8829\n","Epoch 47/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.4607 - loss: 2.4782 - val_accuracy: 0.0530 - val_loss: 10.9619\n","Epoch 48/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.4611 - loss: 2.4666 - val_accuracy: 0.0527 - val_loss: 10.9904\n","Epoch 49/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 69ms/step - accuracy: 0.4718 - loss: 2.4184 - val_accuracy: 0.0519 - val_loss: 11.0875\n","Epoch 50/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.4781 - loss: 2.3964 - val_accuracy: 0.0532 - val_loss: 11.1549\n","Epoch 51/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.4856 - loss: 2.3557 - val_accuracy: 0.0519 - val_loss: 11.2003\n","Epoch 52/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 69ms/step - accuracy: 0.4950 - loss: 2.2916 - val_accuracy: 0.0523 - val_loss: 11.2389\n","Epoch 53/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.4999 - loss: 2.2759 - val_accuracy: 0.0527 - val_loss: 11.3290\n","Epoch 54/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 73ms/step - accuracy: 0.4984 - loss: 2.2683 - val_accuracy: 0.0515 - val_loss: 11.3881\n","Epoch 55/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 69ms/step - accuracy: 0.5151 - loss: 2.2033 - val_accuracy: 0.0507 - val_loss: 11.4317\n","Epoch 56/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.5190 - loss: 2.1677 - val_accuracy: 0.0519 - val_loss: 11.5222\n","Epoch 57/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.5247 - loss: 2.1737 - val_accuracy: 0.0515 - val_loss: 11.5873\n","Epoch 58/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.5364 - loss: 2.1088 - val_accuracy: 0.0507 - val_loss: 11.6552\n","Epoch 59/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.5373 - loss: 2.0973 - val_accuracy: 0.0509 - val_loss: 11.7094\n","Epoch 60/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.5377 - loss: 2.0643 - val_accuracy: 0.0511 - val_loss: 11.7650\n","Epoch 61/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.5433 - loss: 2.0716 - val_accuracy: 0.0488 - val_loss: 11.8188\n","Epoch 62/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.5604 - loss: 2.0009 - val_accuracy: 0.0507 - val_loss: 11.8835\n","Epoch 63/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.5618 - loss: 1.9727 - val_accuracy: 0.0521 - val_loss: 11.9355\n","Epoch 64/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.5649 - loss: 1.9610 - val_accuracy: 0.0499 - val_loss: 11.9738\n","Epoch 65/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 76ms/step - accuracy: 0.5693 - loss: 1.9507 - val_accuracy: 0.0488 - val_loss: 12.0457\n","Epoch 66/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.5752 - loss: 1.9248 - val_accuracy: 0.0507 - val_loss: 12.1064\n","Epoch 67/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 76ms/step - accuracy: 0.5786 - loss: 1.8804 - val_accuracy: 0.0482 - val_loss: 12.1887\n","Epoch 68/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.5865 - loss: 1.8653 - val_accuracy: 0.0478 - val_loss: 12.2100\n","Epoch 69/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.5860 - loss: 1.8514 - val_accuracy: 0.0503 - val_loss: 12.2623\n","Epoch 70/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.5911 - loss: 1.8178 - val_accuracy: 0.0488 - val_loss: 12.3036\n","Epoch 71/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.6031 - loss: 1.7911 - val_accuracy: 0.0521 - val_loss: 12.3990\n","Epoch 72/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.6024 - loss: 1.7603 - val_accuracy: 0.0486 - val_loss: 12.4366\n","Epoch 73/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 78ms/step - accuracy: 0.6111 - loss: 1.7307 - val_accuracy: 0.0490 - val_loss: 12.4862\n","Epoch 74/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 78ms/step - accuracy: 0.6192 - loss: 1.7291 - val_accuracy: 0.0507 - val_loss: 12.5664\n","Epoch 75/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.6121 - loss: 1.7238 - val_accuracy: 0.0511 - val_loss: 12.5896\n","Epoch 76/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.6194 - loss: 1.6902 - val_accuracy: 0.0466 - val_loss: 12.6566\n","Epoch 77/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.6180 - loss: 1.7001 - val_accuracy: 0.0478 - val_loss: 12.7168\n","Epoch 78/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 72ms/step - accuracy: 0.6318 - loss: 1.6324 - val_accuracy: 0.0501 - val_loss: 12.7527\n","Epoch 79/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.6327 - loss: 1.6213 - val_accuracy: 0.0501 - val_loss: 12.8211\n","Epoch 80/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 71ms/step - accuracy: 0.6313 - loss: 1.6181 - val_accuracy: 0.0474 - val_loss: 12.8302\n","Epoch 81/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.6433 - loss: 1.5734 - val_accuracy: 0.0505 - val_loss: 12.8952\n","Epoch 82/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.6402 - loss: 1.5749 - val_accuracy: 0.0453 - val_loss: 12.9152\n","Epoch 83/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.6439 - loss: 1.5651 - val_accuracy: 0.0492 - val_loss: 12.9804\n","Epoch 84/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 74ms/step - accuracy: 0.6482 - loss: 1.5412 - val_accuracy: 0.0503 - val_loss: 13.0347\n","Epoch 85/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 71ms/step - accuracy: 0.6520 - loss: 1.5323 - val_accuracy: 0.0488 - val_loss: 13.1036\n","Epoch 86/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.6539 - loss: 1.5059 - val_accuracy: 0.0488 - val_loss: 13.1201\n","Epoch 87/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.6619 - loss: 1.4950 - val_accuracy: 0.0480 - val_loss: 13.1424\n","Epoch 88/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.6686 - loss: 1.4653 - val_accuracy: 0.0490 - val_loss: 13.1907\n","Epoch 89/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.6731 - loss: 1.4414 - val_accuracy: 0.0480 - val_loss: 13.2619\n","Epoch 90/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 75ms/step - accuracy: 0.6768 - loss: 1.4203 - val_accuracy: 0.0482 - val_loss: 13.2621\n","Epoch 91/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.6823 - loss: 1.4102 - val_accuracy: 0.0470 - val_loss: 13.3246\n","Epoch 92/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 74ms/step - accuracy: 0.6721 - loss: 1.4343 - val_accuracy: 0.0482 - val_loss: 13.4192\n","Epoch 93/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 77ms/step - accuracy: 0.6824 - loss: 1.3944 - val_accuracy: 0.0445 - val_loss: 13.4179\n","Epoch 94/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.6906 - loss: 1.3708 - val_accuracy: 0.0499 - val_loss: 13.4751\n","Epoch 95/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 72ms/step - accuracy: 0.6857 - loss: 1.3726 - val_accuracy: 0.0488 - val_loss: 13.5537\n","Epoch 96/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.6961 - loss: 1.3302 - val_accuracy: 0.0482 - val_loss: 13.5580\n","Epoch 97/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.6882 - loss: 1.3626 - val_accuracy: 0.0488 - val_loss: 13.6176\n","Epoch 98/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.6935 - loss: 1.3425 - val_accuracy: 0.0493 - val_loss: 13.6583\n","Epoch 99/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.6973 - loss: 1.3183 - val_accuracy: 0.0464 - val_loss: 13.7316\n","Epoch 100/100\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.6992 - loss: 1.3176 - val_accuracy: 0.0497 - val_loss: 13.7702\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x7d27855fecc0>"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["history = model.fit(x_train, y_train, epochs = 100, validation_data = (x_test, y_test), verbose = 1)\n","history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJq74irKBKIQ"},"outputs":[],"source":["# Function to predict the next word\n","def predict_next_word(model, tokenizer, text, max_sequence_len):\n","    token_list = tokenizer.texts_to_sequences([text])[0]\n","    if len(token_list) >= max_sequence_len:\n","        token_list = token_list[-(max_sequence_len-1):]  # Ensure the sequence length matches max_sequence_len-1\n","    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","    predicted = model.predict(token_list, verbose=0)\n","    predicted_word_index = np.argmax(predicted, axis=1)\n","    for word, index in tokenizer.word_index.items():\n","        if index == predicted_word_index:\n","            return word\n","    return None"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":669,"status":"ok","timestamp":1758828865079,"user":{"displayName":"Avanti Sharma","userId":"17226617004774171125"},"user_tz":-330},"id":"KKcP1dQABJ5Q","outputId":"cca3766a-8538-4428-bed5-6308fe4dcf39"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input text:To be or not to be\n","Next Word PRediction:that\n"]}],"source":["input_text=\"To be or not to be\"\n","print(f\"Input text:{input_text}\")\n","max_sequence_len=model.input_shape[1]+1\n","next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n","print(f\"Next Word PRediction:{next_word}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":148,"status":"ok","timestamp":1758829166489,"user":{"displayName":"Avanti Sharma","userId":"17226617004774171125"},"user_tz":-330},"id":"iOeh4CoHBJ1w","outputId":"9c7e45ba-05f4-450a-ccdd-984793d09bdc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Next Word PRediction:lordship\n"]}],"source":["input_text=\"Osr. I commend my duty to your\"\n","max_sequence_len=model.input_shape[1]+1\n","next_word=predict_next_word(model,tokenizer,input_text,max_sequence_len)\n","print(f\"Next Word PRediction:{next_word}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341},"executionInfo":{"elapsed":54,"status":"ok","timestamp":1758829357368,"user":{"displayName":"Avanti Sharma","userId":"17226617004774171125"},"user_tz":-330},"id":"cY17TZMCBLEG","outputId":"5950f231-7055-4470-de16-12218860067c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"sequential_5\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["## Define the model\n","from tensorflow.keras.layers import GRU\n","model=Sequential()\n","model.add(Embedding(total_words,100,input_length=max_sequence_len-1))\n","model.add(GRU(150,return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(GRU(100))\n","model.add(Dense(total_words,activation=\"softmax\"))\n","\n","# #Compile the model\n","model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"-RryuTZ-ZcDd","outputId":"d524147a-046f-44cb-d362-068b8e7006ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 69ms/step - accuracy: 0.0303 - loss: 7.2068 - val_accuracy: 0.0334 - val_loss: 6.7768\n","Epoch 2/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.0406 - loss: 6.4659 - val_accuracy: 0.0493 - val_loss: 6.7254\n","Epoch 3/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.0576 - loss: 6.1799 - val_accuracy: 0.0657 - val_loss: 6.6586\n","Epoch 4/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 68ms/step - accuracy: 0.0688 - loss: 5.8975 - val_accuracy: 0.0732 - val_loss: 6.7037\n","Epoch 5/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.0829 - loss: 5.5911 - val_accuracy: 0.0740 - val_loss: 6.7315\n","Epoch 6/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 75ms/step - accuracy: 0.0989 - loss: 5.2848 - val_accuracy: 0.0705 - val_loss: 6.8392\n","Epoch 7/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 69ms/step - accuracy: 0.1096 - loss: 5.0139 - val_accuracy: 0.0736 - val_loss: 6.9850\n","Epoch 8/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 68ms/step - accuracy: 0.1288 - loss: 4.7418 - val_accuracy: 0.0705 - val_loss: 7.0502\n","Epoch 9/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 72ms/step - accuracy: 0.1524 - loss: 4.4933 - val_accuracy: 0.0686 - val_loss: 7.1838\n","Epoch 10/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.1865 - loss: 4.2593 - val_accuracy: 0.0699 - val_loss: 7.2675\n","Epoch 11/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 73ms/step - accuracy: 0.2150 - loss: 4.0742 - val_accuracy: 0.0664 - val_loss: 7.4124\n","Epoch 12/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.2450 - loss: 3.8586 - val_accuracy: 0.0676 - val_loss: 7.5077\n","Epoch 13/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 71ms/step - accuracy: 0.2765 - loss: 3.6656 - val_accuracy: 0.0653 - val_loss: 7.6040\n","Epoch 14/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 67ms/step - accuracy: 0.3051 - loss: 3.5106 - val_accuracy: 0.0653 - val_loss: 7.6994\n","Epoch 15/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.3356 - loss: 3.3211 - val_accuracy: 0.0643 - val_loss: 7.7988\n","Epoch 16/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.3567 - loss: 3.1885 - val_accuracy: 0.0620 - val_loss: 7.9024\n","Epoch 17/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.3748 - loss: 3.0489 - val_accuracy: 0.0620 - val_loss: 7.9865\n","Epoch 18/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.4050 - loss: 2.9256 - val_accuracy: 0.0628 - val_loss: 8.0542\n","Epoch 19/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.4217 - loss: 2.7842 - val_accuracy: 0.0616 - val_loss: 8.1723\n","Epoch 20/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 67ms/step - accuracy: 0.4368 - loss: 2.7128 - val_accuracy: 0.0633 - val_loss: 8.2194\n","Epoch 21/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.4586 - loss: 2.5881 - val_accuracy: 0.0616 - val_loss: 8.2990\n","Epoch 22/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.4723 - loss: 2.5113 - val_accuracy: 0.0606 - val_loss: 8.4026\n","Epoch 23/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.4892 - loss: 2.4438 - val_accuracy: 0.0614 - val_loss: 8.4743\n","Epoch 24/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.5123 - loss: 2.3392 - val_accuracy: 0.0583 - val_loss: 8.5391\n","Epoch 25/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.5164 - loss: 2.2650 - val_accuracy: 0.0591 - val_loss: 8.6499\n","Epoch 26/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.5403 - loss: 2.1803 - val_accuracy: 0.0561 - val_loss: 8.6869\n","Epoch 27/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.5529 - loss: 2.1141 - val_accuracy: 0.0585 - val_loss: 8.7676\n","Epoch 28/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 69ms/step - accuracy: 0.5591 - loss: 2.0487 - val_accuracy: 0.0581 - val_loss: 8.8155\n","Epoch 29/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.5712 - loss: 1.9971 - val_accuracy: 0.0558 - val_loss: 8.9097\n","Epoch 30/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.5782 - loss: 1.9467 - val_accuracy: 0.0573 - val_loss: 8.9839\n","Epoch 31/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.5971 - loss: 1.8621 - val_accuracy: 0.0569 - val_loss: 9.0457\n","Epoch 32/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.6010 - loss: 1.8348 - val_accuracy: 0.0593 - val_loss: 9.1008\n","Epoch 33/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.6110 - loss: 1.7971 - val_accuracy: 0.0552 - val_loss: 9.1705\n","Epoch 34/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 67ms/step - accuracy: 0.6180 - loss: 1.7502 - val_accuracy: 0.0563 - val_loss: 9.2204\n","Epoch 35/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 69ms/step - accuracy: 0.6285 - loss: 1.6871 - val_accuracy: 0.0558 - val_loss: 9.3151\n","Epoch 36/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.6352 - loss: 1.6643 - val_accuracy: 0.0563 - val_loss: 9.3775\n","Epoch 37/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 69ms/step - accuracy: 0.6453 - loss: 1.6180 - val_accuracy: 0.0595 - val_loss: 9.4374\n","Epoch 38/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.6511 - loss: 1.5858 - val_accuracy: 0.0565 - val_loss: 9.4850\n","Epoch 39/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.6600 - loss: 1.5412 - val_accuracy: 0.0571 - val_loss: 9.5356\n","Epoch 40/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.6650 - loss: 1.5098 - val_accuracy: 0.0571 - val_loss: 9.5911\n","Epoch 41/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 72ms/step - accuracy: 0.6752 - loss: 1.4710 - val_accuracy: 0.0577 - val_loss: 9.6587\n","Epoch 42/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.6796 - loss: 1.4531 - val_accuracy: 0.0517 - val_loss: 9.7077\n","Epoch 43/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.6860 - loss: 1.4262 - val_accuracy: 0.0573 - val_loss: 9.7698\n","Epoch 44/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.6899 - loss: 1.3849 - val_accuracy: 0.0540 - val_loss: 9.8317\n","Epoch 45/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.6988 - loss: 1.3469 - val_accuracy: 0.0565 - val_loss: 9.8827\n","Epoch 46/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.6993 - loss: 1.3462 - val_accuracy: 0.0517 - val_loss: 9.9316\n","Epoch 47/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.7028 - loss: 1.3215 - val_accuracy: 0.0505 - val_loss: 10.0007\n","Epoch 48/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 68ms/step - accuracy: 0.7067 - loss: 1.2960 - val_accuracy: 0.0538 - val_loss: 10.0468\n","Epoch 49/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 70ms/step - accuracy: 0.7046 - loss: 1.2947 - val_accuracy: 0.0550 - val_loss: 10.0884\n","Epoch 50/50\n","\u001b[1m644/644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 69ms/step - accuracy: 0.7147 - loss: 1.2661 - val_accuracy: 0.0517 - val_loss: 10.1443\n"]}],"source":["history3 = model.fit(x_train, y_train, epochs = 50, validation_data = (x_test, y_test), verbose = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pF_-RwwPah8o"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZn5dDcvTQIl09UXuL4RZO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}